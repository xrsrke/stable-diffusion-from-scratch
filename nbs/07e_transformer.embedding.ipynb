{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45502ac5-de74-4fae-b5de-aa8ebc060d5c",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb9152-a1b9-4746-9a47-60a52710bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformer.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d23d9-9878-4bbf-9c06-680e3e175fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46c01c-9a4d-481e-996c-a317771af931",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34549103-f4d4-4b45-8097-4f284cf7b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d5077-2127-4ae7-93d4-2ee48635908e",
   "metadata": {},
   "source": [
    "### Text Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c87b8-6200-4295-9949-7694cb7bfadc",
   "metadata": {},
   "source": [
    "one for source, one for target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfeed58-a9cd-443e-ace6-6ae04192e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, padding_idx, d_model: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # shape(x) = [batch_size x seq_len]\n",
    "        # shape(embedding) = [batch_size x seq_len x d_model]\n",
    "        embedding = self.embed(x)\n",
    "        return embedding * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc7d4e-a7fa-4ce3-a00a-ee43cacbf4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(5, 4, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073034e3-9a5c-4033-8ac1-d169efe1a8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8024,  0.3854, -0.9739,  2.5922],\n",
       "        [ 3.3359, -1.2493,  0.0466,  1.5786],\n",
       "        [ 1.7705, -1.1482,  0.0487,  0.8839],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed(torch.tensor([1, 2, 3, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0714b-aa79-4839-80a5-56fedc3c12ba",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073c480-745b-4eff-b01c-22cfd483ed7d",
   "metadata": {},
   "source": [
    "$P E_{p o s, 2 i}=\\sin \\left(\\frac{p o s}{10000^{\\frac{2 i}{d}}}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81b426-f7ae-4f8b-a61d-1bd25814efa1",
   "metadata": {},
   "source": [
    "$P E_{p o s, 2 i+1}=\\cos \\left(\\frac{p o s}{10000^{\\frac{2 i}{d}}}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e15098-0ac8-48b7-950f-2a556c59f4ca",
   "metadata": {},
   "source": [
    "- `pos`: the position of a word in a sequence\n",
    "- `i`: is the index in the word encoding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3a376-c17c-4d72-9fb2-759ed425689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.3, max_seq_len : float = 2000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        two_i = torch.arange(0, d_model, step=2).float()\n",
    "        div_term = torch.pow(1000, (two_i/torch.tensor([d_model]))).float()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(pos/div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos/div_term)\n",
    "        \n",
    "        # add one dim for batch_size\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x is text embedding\n",
    "        # shape(x) = [batch_size x seq_len x d_model]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        # extract the position for seq_len\n",
    "        pe = self.pe[:, :seq_len].detach()\n",
    "        \n",
    "        x = x.add(pe)\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441de25b-ea13-484e-b7b4-a6f5144168b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = PositionalEncoding(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4a55d-bc44-4756-9772-b20c6a1ee0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2000, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position.pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d0096-1f04-456e-978c-654fe948acfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.8415,  0.5403,  0.1769,  ...,  0.9995,  0.0056,  1.0000],\n",
       "         [ 0.9093, -0.4161,  0.3482,  ...,  0.9980,  0.0112,  0.9999],\n",
       "         ...,\n",
       "         [-0.8689,  0.4950, -0.1221,  ...,  0.9496, -0.9727,  0.2322],\n",
       "         [-0.0529,  0.9986, -0.2958,  ...,  0.9392, -0.9713,  0.2377],\n",
       "         [ 0.8117,  0.5841, -0.4601,  ...,  0.9279, -0.9700,  0.2432]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a105f4-fc8e-4161-bec8-65dc66f0d38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca4883-9d0b-4ac3-a3e7-7543b407d994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:48:25) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d964ee95f874ee5744c8d970377d82f548475fce4b579cb2112551caf7b0ce8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
