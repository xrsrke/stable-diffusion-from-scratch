{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Attention Mechanism\n",
    "\n",
    "> An efficient implementation of attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformer.efficient_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_head: int):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head\n",
    "    \n",
    "    def forward(self, q_batch: torch.Tensor, k_batch: torch.Tensor, v_batch: torch.Tensor, mask=None):\n",
    "        # shape(q_batch) = [batch_size x num_heads x Q_len x d_head]\n",
    "        # shape(k_batch, v_batch) = [batch_size x num_heads x KV_len x d_head]\n",
    "        \n",
    "        # shape(K_permuted) = [batch_size x num_heads x d_head x length]\n",
    "        k_batch_permuted = k_batch.permute(0, 1, 3, 2)\n",
    "        q_k_matmul_batch = torch.matmul(q_batch, k_batch_permuted)\n",
    "        scores_batch = q_k_matmul_batch / math.sqrt(self.d_head)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores_batch = scores_batch.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights_batch = F.softmax(scores_batch, dim=-1)\n",
    "        output_batch = torch.matmul(attention_weights_batch, v_batch)\n",
    "        \n",
    "        return output_batch, attention_weights_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_model // n_head\n",
    "        \n",
    "        self.attention = ScaleDotProductAttention(d_head = self.d_head)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.linear_projection = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_head(self, x: torch.Tensor):\n",
    "        batch_size, n_words, d_model = x.size()\n",
    "        d_head = d_model // self.n_head\n",
    "        \n",
    "        tenxsor = x.view(batch_size, n_words, self.n_head, d_head)\n",
    "        x = x.transpose(2, 1) # transpose self.n_head and n_words\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def concat(self, x: torch.Tensor):\n",
    "        batch_size, n_heads, n_words, d_head = x.size()\n",
    "        return x.view(batch_size, -1, self.d_model)\n",
    "    \n",
    "    def forward(self, pre_q: torch.Tensor, pre_k: torch.Tensor, pre_v: torch.Tensor, mask=None):\n",
    "        \n",
    "        # 1. dot product with weight matrices\n",
    "        q_batch, k_batch, v_batch = self.w_q(pre_q), self.w_k(pre_k), self.w_v(pre_v)\n",
    "        \n",
    "        # 2. split tensor by number of heads\n",
    "        q_batch, k_batch, v_batch = self.split_head(q_batch), self.split_head(k_batch), self.split_head(v_batch)\n",
    "        \n",
    "        output_batch, attention_weights_batch = self.attention(q_batch, k_batch, v_batch, mask)\n",
    "        output_batch = self.concat(output_batch)\n",
    "        \n",
    "        projection_batch = self.dropout(self.linear_projection(output_batch))\n",
    "        \n",
    "        return projection_batch, attention_weights_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:48:25) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d964ee95f874ee5744c8d970377d82f548475fce4b579cb2112551caf7b0ce8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
