{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae69f1aa-d914-4a38-afd8-e066ad964b77",
   "metadata": {},
   "source": [
    "# Encoder Layer\n",
    "\n",
    "> Implement Transformer's Encoder Layer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f816c-d1bd-493d-a236-b9e0677701ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformer.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606fa47d-cc4e-4aab-adac-d867ddcd98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485e864-a61a-41e4-bcca-7447cd02daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9b8b6-d351-44af-bf7b-6077fb23242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from foundation.transformer.attention import MultiHeadAttention\n",
    "from foundation.transformer.efficient_attention import MultiHeadAttention\n",
    "from foundation.transformer.embedding import TextEmbedding\n",
    "from foundation.transformer.positional_encoding import PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b0785-76f3-461b-b964-2e1fb3a14ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ResidualLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: Optional[float] = 0.3):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, residual: torch.Tensor):\n",
    "        return self.layer_norm(self.dropout(x + residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552731f9-6aa5-4e28-81f0-f1dd86592f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PostionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: Optional[float] = 0.3):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            d_model (int): the dimension of text embedding\n",
    "            d_ff (int): the hidden dimension of the feed forward linear layer\n",
    "            dropout (float, optional): dropout. Defaults to 0.3.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # shape(x) = [batch_size x seq_len x d_model]\n",
    "\n",
    "        # shape(output) = [batch_size x seq_len x d_model]\n",
    "        output = self.feed_forward(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e08e5-fc4a-41a9-b325-bc95762e1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: Optional[float] = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm_1 = ResidualLayerNorm(d_model, dropout)\n",
    "        self.feed_forward = PostionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm_2 = ResidualLayerNorm(d_model, dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask = None):\n",
    "        \n",
    "        # shape(mha) = [batch_size x seq_len x d_model]\n",
    "        # shape(encoder_attention_weights) = [batch_size x n_heads x seq_len x seq_len]\n",
    "        mha, encoder_attention_weights = self.mha(x, x, x, mask=mask)\n",
    "        \n",
    "        # shape(norm1) = [batch_size x seq_len x d_model]\n",
    "        norm_1 = self.norm_1(mha, x)\n",
    "        \n",
    "        # shape(feed_forward) = [batch_size x seq_len x d_model]\n",
    "        feed_forward = self.feed_forward(norm_1)\n",
    "        \n",
    "        # shape(output) = [batch_size x seq_len x d_model]\n",
    "        output = self.norm_2(feed_forward, norm_1)\n",
    "        \n",
    "        return output, encoder_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42152174-4001-4123-80d2-1dc69c324cd9",
   "metadata": {},
   "source": [
    "`num_heads`: t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c3c48-3b2e-4b02-9b89-41905d06ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model: int, n_heads: int, n_layers: int,\n",
    "        d_ff: int, dropout: Optional[float] = 0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = TextEmbedding(\n",
    "            vocab_size = 10000,\n",
    "            d_model = d_model,\n",
    "            padding_idx = 0\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                d_model, n_heads, d_ff, dropout\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask = None):\n",
    "        # shape(x) = [batch_size x src_seq_len]\n",
    "        \n",
    "        # shape(embeddings) = [batch_size x src_seq_len x d_model]\n",
    "        embeddings = self.embedding(x)\n",
    "        # shape(encoding) = [batch_size x src_seq_len x d_model]\n",
    "        encoding = self.positional_encoding(embeddings)\n",
    "        \n",
    "        for encoder in self.encoders:\n",
    "            # shape(encoding) = [batch_size x src_seq_len x d_model]\n",
    "            # shape(encoder_attention_weights) = [batch_size x n_heads x src_seq_len x src_seq_len]\n",
    "            encoding, encoder_attention_weights = encoder(encoding, mask)\n",
    "        \n",
    "        return encoding, encoder_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1b150-4a4b-4f48-a443-8e03fed63741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
