{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9f6b49-600b-4a3a-a917-0faa3d173353",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "> Implement Transformer's Encoder Layer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b26c1b-e0e1-4efb-81ba-9e418e4eae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformer.positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f71f46-37f1-430f-9bc2-9d9e56af7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7bbd3c-5f7a-4810-ac84-f49de78168df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d8b260-04c0-497b-910b-42a71fac22b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f07966b",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "$P E_{p o s, 2 i}=\\sin \\left(\\frac{p o s}{10000^{\\frac{2 i}{d}}}\\right)$\n",
    "$P E_{p o s, 2 i+1}=\\cos \\left(\\frac{p o s}{10000^{\\frac{2 i}{d}}}\\right)$\n",
    "- `pos`: the position of a word in a sequence\n",
    "- `i`: is the index in the word encoding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "333f1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_len : float = 2000, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.pe = torch.zeros(max_seq_len, d_model)\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        two_i = torch.arange(0, d_model, step=2).float()\n",
    "        div_term = torch.pow(1000, (two_i/torch.tensor([d_model]))).float()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(pos/div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos/div_term)\n",
    "        \n",
    "        # add one dim for batch_size\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x is text embedding\n",
    "        # shape(x) = [batch_size x seq_len x d_model]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        # extract the position for seq_len\n",
    "        pe = self.pe[:, :seq_len].detach()\n",
    "        \n",
    "        x = x.add(pe)\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0ce9083",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'pe' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m position \u001b[39m=\u001b[39m PositionalEncoding(\u001b[39m8\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m position\u001b[39m.\u001b[39mpe\u001b[39m.\u001b[39mshape\n\u001b[1;32m      3\u001b[0m position\u001b[39m.\u001b[39mpe\n",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m, in \u001b[0;36mPositionalEncoding.__init__\u001b[0;34m(self, d_model, max_seq_len, dropout)\u001b[0m\n\u001b[1;32m     11\u001b[0m two_i \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, d_model, step\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     12\u001b[0m div_term \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpow(\u001b[39m1000\u001b[39m, (two_i\u001b[39m/\u001b[39mtorch\u001b[39m.\u001b[39mtensor([d_model])))\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 14\u001b[0m pe[:, \u001b[39m0\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msin(pos\u001b[39m/\u001b[39mdiv_term)\n\u001b[1;32m     15\u001b[0m pe[:, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(pos\u001b[39m/\u001b[39mdiv_term)\n\u001b[1;32m     17\u001b[0m \u001b[39m# add one dim for batch_size\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pe' referenced before assignment"
     ]
    }
   ],
   "source": [
    "position = PositionalEncoding(8)\n",
    "position.pe.shape\n",
    "position.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dca5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.randn(10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b3c512",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m position(tokens)\n",
      "File \u001b[0;32m~/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# extract the position for seq_len\u001b[39;00m\n\u001b[1;32m     28\u001b[0m pe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpe[:, :seq_len]\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m---> 30\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49madd(pe)\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "position(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2412758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d964ee95f874ee5744c8d970377d82f548475fce4b579cb2112551caf7b0ce8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
