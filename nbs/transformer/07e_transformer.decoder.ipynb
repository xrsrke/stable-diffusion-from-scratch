{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d04140f-e2f7-4c53-b87d-856bbe9c8432",
   "metadata": {},
   "source": [
    "# Decoder Layer\n",
    "\n",
    "> Implement Transformer's Decoder from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930b1ca-bfa1-4ef6-80ea-1b34807f7da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformer.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853a44a-401e-4373-96f2-3e75ef0d73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27893829-e525-4b15-8177-852bb2ab4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a6e67-1898-446d-a9c9-1cd4d02a7460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn\n",
    "from foundation.transformer.efficient_attention import MultiHeadAttention\n",
    "from foundation.transformer.encoder import PostionWiseFeedForward\n",
    "from foundation.transformer.embedding import TextEmbedding\n",
    "from foundation.transformer.positional_encoding import PositionalEncoding\n",
    "from foundation.transformer.encoder import ResidualLayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2450a499-74ff-45a2-8679-15ac47dd113e",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebc3a5-445d-45e6-a788-d450cbe8ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_mask(size):\n",
    "    mask = torch.ones((1, size, size)).triu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af8aa39-2a01-49a9-b87a-c5adeaea3859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [0., 1., 1., 1.],\n",
       "         [0., 0., 1., 1.],\n",
       "         [0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((1, 4, 4)).triu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc13924-086d-4618-9d1a-287ac109be2f",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e07e62-2946-41fe-8ed6-f8a47f4fea49",
   "metadata": {},
   "source": [
    "So, query comes from the current decoder layer. key, value come from the last encoder layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91385e94-9bfe-451e-ad37-1c72400ed03f",
   "metadata": {},
   "source": [
    "- `x`: the current input for the decoder\n",
    "    + already through embedding and positional encoding\n",
    "- `encoder_outputs`:\n",
    "- `trg_mask`: target mask for all illegal positions\n",
    "- `src_mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d4812-9a82-4153-8c67-fca36de293fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DecoderLayer(nn.ModuleList):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout:float=0.3):\n",
    "        super().__init__()\n",
    "        self.norm_1 = ResidualLayerNorm(d_model)\n",
    "        self.norm_2 = ResidualLayerNorm(d_model)\n",
    "        self.norm_3 = ResidualLayerNorm(d_model)\n",
    "        \n",
    "        self.masked_mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.encoder_decoder_mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = PostionWiseFeedForward(d_model, d_ff)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor, encoder_output: torch.Tensor,\n",
    "        trg_mask: torch.Tensor, src_mask: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): the input for the decoder\n",
    "            encoder_output (_type_): the ouput of the encoder stack\n",
    "            trg_mask (_type_): the mask for target sequence\n",
    "            src_mask (_type_): the mask for source sequence\n",
    "        \"\"\"\n",
    "        # shape(x) = [batch_size x trg_seq_len x d_model]\n",
    "        # shape(encoder_output) = [batch_size x src_seq_len x d_model]\n",
    "        \n",
    "        \n",
    "        # shape(masked_mha) = [batch_size x trg_se_len x d_model]\n",
    "        # shape(masked_mha_attn_weights) \n",
    "        # = [batch_size x n_heads x trg_seq_len x trg_seq_len]\n",
    "        masked_mha, masked_mha_attn_weights = self.masked_mha(x, x, x, mask=trg_mask)\n",
    "        \n",
    "        norm_1 = self.norm_1(masked_mha, x)\n",
    "        \n",
    "        # shape(mha) = [batch_size x trg_seq_len x d_model]\n",
    "        # shape(mha_attn_weights) = [batch_size x n_heads x trg_seq_len x trg_seq_len]\n",
    "        encoder_decoder_mha, encoder_decoder_mha_attn_weights = self.encoder_decoder_mha(\n",
    "            pre_q=norm_1, pre_k=encoder_output, pre_v=encoder_output,\n",
    "            mask=src_mask\n",
    "        )\n",
    "        \n",
    "        norm_2 = self.norm_2(encoder_decoder_mha, norm_1)\n",
    "        \n",
    "        feed_forward = self.feed_forward(norm_2)\n",
    "        \n",
    "        norm_3 = self.norm_3(feed_forward, norm_2)\n",
    "        \n",
    "        return norm_3, masked_mha_attn_weights, encoder_decoder_mha_attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2fa42-26cc-49e4-9b45-e21c0be08430",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd413c21-536b-4d93-a882-961a449a1051",
   "metadata": {},
   "source": [
    "- `x` is the raw tokenized input\n",
    "- `encoder_output`: the ouput of encoder\n",
    "- `trg_mask`\n",
    "- `srg_mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df041ffa-ade2-4152-954b-5d47ace7caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model: int,\n",
    "        n_heads: int, n_layers: int, d_ff: int,\n",
    "        dropout: float=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = TextEmbedding(vocab_size=1000, d_model=d_model, padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderLayer(\n",
    "                d_model,\n",
    "                n_heads,\n",
    "                d_ff,\n",
    "                dropout\n",
    "            ) for layer in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, trg_mask: torch.Tensor, src_mask: torch.Tensor):\n",
    "        # shape(x) = [batch_size x trg_seq_len], raw tokenizer input\n",
    "        \n",
    "        # shape(embeddings) = [batch_size x trg_seq_len x d_model]\n",
    "        embeddings = self.embedding(x)\n",
    "        # shape(encoding) = [batch_size x trg_seq_len x d_model]\n",
    "        encoding = self.positional_encoding(embeddings)\n",
    "        \n",
    "        for decoder in self.decoders:\n",
    "            # shape(encoding) = [batch_size x trg_seq_len x d_model]\n",
    "            # shape(masked_mha_attn_weights) = [batch_size x num_heads x trg_seq_len x trg_seq_len]\n",
    "            # shape(mha_attn_weights) = [batch_size x num_heads x trg_seq_len x src_seq_len]\n",
    "            encoding, masked_mha_attn_weights, mha_attn_weights = decoder(\n",
    "                encoding, encoder_output,\n",
    "                trg_mask, src_mask\n",
    "            )\n",
    "        \n",
    "        return encoding, masked_mha_attn_weights, mha_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c2ab0-c2c0-4f4f-8e23-fbbd51a1db58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
