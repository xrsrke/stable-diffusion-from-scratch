{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9f6b49-600b-4a3a-a917-0faa3d173353",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "> Implement Transformer's Encoder Layer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b26c1b-e0e1-4efb-81ba-9e418e4eae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformer.positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f71f46-37f1-430f-9bc2-9d9e56af7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bbd3c-5f7a-4810-ac84-f49de78168df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d8b260-04c0-497b-910b-42a71fac22b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f07966b",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "$P E_{p o s, 2 i}=\\sin \\left(\\frac{p o s}{10000^{\\frac{2 i}{d}}}\\right)$\n",
    "$P E_{p o s, 2 i+1}=\\cos \\left(\\frac{p o s}{10000^{\\frac{2 i}{d}}}\\right)$\n",
    "- `pos`: the position of a word in a sequence\n",
    "- `i`: is the index in the word encoding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq_len : float = 2000, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pos = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        two_i = torch.arange(0, d_model, step=2).float()\n",
    "        div_term = torch.pow(1000, (two_i/torch.tensor([d_model]))).float()\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(pos/div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos/div_term)\n",
    "        \n",
    "        # add one dim for batch_size\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x is text embedding\n",
    "        # shape(x) = [batch_size x seq_len x d_model]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        # extract the position for seq_len\n",
    "        pe = self.pe[:, :seq_len].detach()\n",
    "        \n",
    "        x = x.add(pe)\n",
    "        \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce9083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.0000,  0.0000,  ...,  1.0000,  0.0000,  1.0000],\n",
       "         [ 0.8415,  0.5403,  0.1769,  ...,  0.9995,  0.0056,  1.0000],\n",
       "         [ 0.9093, -0.4161,  0.3482,  ...,  0.9980,  0.0112,  0.9999],\n",
       "         ...,\n",
       "         [-0.8689,  0.4950, -0.1221,  ...,  0.9496, -0.9727,  0.2322],\n",
       "         [-0.0529,  0.9986, -0.2958,  ...,  0.9392, -0.9713,  0.2377],\n",
       "         [ 0.8117,  0.5841, -0.4601,  ...,  0.9279, -0.9700,  0.2432]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = PositionalEncoding(8)\n",
    "position.pe.shape\n",
    "position.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.randn(10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3c512",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m position(tokens)\n",
      "File \u001b[0;32m~/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39m# extract the position for seq_len\u001b[39;00m\n\u001b[1;32m     28\u001b[0m pe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpe[:, :seq_len]\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m---> 30\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49madd(pe)\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (8) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "position(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2412758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
