{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae69f1aa-d914-4a38-afd8-e066ad964b77",
   "metadata": {},
   "source": [
    "# Encoder Layer\n",
    "\n",
    "> Implement Transformer's Encoder Layer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f816c-d1bd-493d-a236-b9e0677701ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformer.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606fa47d-cc4e-4aab-adac-d867ddcd98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f485e864-a61a-41e4-bcca-7447cd02daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e9b8b6-d351-44af-bf7b-6077fb23242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from foundation.transformer.attention import MultiHeadAttention\n",
    "from foundation.transformer.positional_encoding import PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86b0785-76f3-461b-b964-2e1fb3a14ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ResidualLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, residual: torch.Tensor):\n",
    "        return self.layer_norm(self.dropout(x + residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552731f9-6aa5-4e28-81f0-f1dd86592f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class PostionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # shape(x) = [B x seq_len x D]\n",
    "\n",
    "        output = self.feed_forward(x)\n",
    "        # shape(output) = [B x seq_len x D]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e08e5-fc4a-41a9-b325-bc95762e1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.norm_1 = ResidualLayerNorm(d_model, dropout)\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm_2 = ResidualLayerNorm(d_model, dropout)\n",
    "        # self.positional_wise = None\n",
    "        self.feed_forward = PostionWiseFeedForward(d_model, d_ff, dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask):\n",
    "        \n",
    "        # shape(mha) = [batch_size x seq_len x d_model]\n",
    "        # shape(encoder_attention_weights) = [batch_size x num_heads x seq_len x seq_len]\n",
    "        mha, encoder_attention_weights = self.mha(x, x, x, mask=mask)\n",
    "        \n",
    "        # shape(norm1) = [batch_size x seq_len x d_model]\n",
    "        norm_1 = self.norm_1(mha, x)\n",
    "        \n",
    "        # shape(feed_forward) = [batch_size x seq_len x d_model]\n",
    "        feed_forward = self.feed_forward(norm_1)\n",
    "        \n",
    "        # shape(norm_2) = [batch_size x seq_len x d_model]\n",
    "        norm_2 = self.norm_2(feed_forward, norm_1)\n",
    "        \n",
    "        return norm_2, encoder_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42152174-4001-4123-80d2-1dc69c324cd9",
   "metadata": {},
   "source": [
    "`num_heads`: t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c3c48-3b2e-4b02-9b89-41905d06ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding: Callable, d_model: int, num_heads: int, num_layers: int,\n",
    "        d_ff: int, dropout: float = 0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                d_model, num_heads, d_ff, dropout\n",
    "            ) for layer in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # shape(x) = [batch_size x src_seq_len]\n",
    "        \n",
    "        # shape(embeddings) = [batch_size x src_seq_len x d_model]\n",
    "        embeddings = self.embedding(x)\n",
    "        # shape(encoding) = [batch_size x src_seq_len x d_model]\n",
    "        encoding = self.positional_encoding(embeddings)\n",
    "        \n",
    "        for encoder in self.encoders:\n",
    "            # shape(encoding) = [batch_size x src_seq_len x d_model]\n",
    "            # shape(encoder_attention_weights) = [batch_size x num_heads x src_seq_len x src_seq_len]\n",
    "            encoding, encoder_attention_weights = encoder(encoding, mask)\n",
    "        \n",
    "        return encoding, encoder_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1b150-4a4b-4f48-a443-8e03fed63741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:48:25) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d964ee95f874ee5744c8d970377d82f548475fce4b579cb2112551caf7b0ce8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
