{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp stable_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from foundation.autoencoder.autoencoder_kl import AutoencoderKL\n",
    "from foundation.clip.tokenizier import CLIPTokenizer\n",
    "from foundation.clip.text_encoder import CLIPTextEncoder\n",
    "from foundation.unet.condition_model import UNet2DConditionModel\n",
    "from foundation.scheduler.lms import LMSDiscreteScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class StableDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_inference_steps: int = 30, guidance_scale: float = 7.5,\n",
    "        seed: int = 42\n",
    "    ):\n",
    "        self.tokenizer = CLIPTokenizer()\n",
    "        self.text_encoder = CLIPTextEncoder()\n",
    "        self.unet = UNet2DConditionModel()\n",
    "        self.image_decoder = AutoencoderKL()\n",
    "        self.scheduler = LMSDiscreteScheduler()\n",
    "        \n",
    "        self.n_inference_steps: int = n_inference_steps\n",
    "        self.guidance_scale: float = guidance_scale\n",
    "        self.seed: int = seed\n",
    "        \n",
    "        self.batch_size = 1\n",
    "        \n",
    "        self.height = 512\n",
    "        self.width = 512\n",
    "        self.latent_height = 512 // 8\n",
    "        self.latent_width = 512 // 8\n",
    "        \n",
    "        self.scheduler.set_timesteps(self.n_inference_steps)\n",
    "    \n",
    "    def create_text_embeddings(self, prompt: str) -> torch.Tensor:\n",
    "        prompt_tokens = self.tokenizer.encode(prompt)\n",
    "        max_length = prompt_tokens.input_ids.shape[-1]\n",
    "        \n",
    "        uncond_tokens = self.tokenizer(\n",
    "            [\"\"] * self.batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prompt_embeddings = self.text_encoder.encode_from_ids(prompt_tokens.input_ids)\n",
    "            uncond_embeddings = self.text_encoder.encode_from_ids(uncond_tokens.input_ids)\n",
    "        \n",
    "        text_embeddings = torch.cat([uncond_embeddings, prompt_embeddings])\n",
    "        \n",
    "        return text_embeddings\n",
    "    \n",
    "    def generate_random_latent(self) -> torch.Tensor:\n",
    "        latents = torch.randn((\n",
    "            self.batch_size, self.unet.in_channels,\n",
    "            self.latent_height, self.latent_width\n",
    "        ))\n",
    "        latents = latents * self.scheduler.init_noise_sigma\n",
    "        return latents\n",
    "    \n",
    "    def forward_diffusion(self, text_embeddings: torch.Tensor, latents: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            for i, t in tqdm(enumerate(self.scheduler.timesteps)):\n",
    "                # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                latent_model_input = torch.cat([latents] * 2)\n",
    "                sigma = self.scheduler.sigmas[i]\n",
    "                \n",
    "                # scale the latents (preconditions)\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "                \n",
    "                # predict the noise residual\n",
    "                with torch.no_grad():\n",
    "                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "                    \n",
    "                # perform guidance\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                # latents = scheduler.step(noise_pred, i, latents)[\"prev_sample\"] # Diffusers 0.3 and below\n",
    "                latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "                \n",
    "        return latents\n",
    "        \n",
    "    def decode_latents(self, latents: torch.tensor):\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        with torch.no_grad():\n",
    "            image = self.image_decoder.decode(latents).sample\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def decode_images_from_vae(self, image: torch.Tensor):\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        images = (image * 255).round().astype(\"uint8\")\n",
    "        \n",
    "        return images\n",
    "        \n",
    "    def generate(self, prompt: str):        \n",
    "        text_embeddings = self.create_text_embeddings(prompt)\n",
    "        latents = self.generate_random_latent()\n",
    "        latents = self.forward_diffusion(text_embeddings, latents)\n",
    "        images = self.decode_latents(latents)\n",
    "        images = self.decode_images_from_vae(images)\n",
    "        \n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = StableDiffusion(n_inference_steps=30)\n",
    "# images = model.generate(prompt=\"Chibi spiderman, octane rendering, modern Disney style\")\n",
    "# pil_images = [Image.fromarray(image) for image in images]\n",
    "# pil_images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
