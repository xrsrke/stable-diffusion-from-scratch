{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Tokenizier\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp clip.tokenizier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import re\n",
    "import os\n",
    "import gzip\n",
    "from functools import lru_cache\n",
    "from collections import namedtuple\n",
    "\n",
    "import transformers\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to build a CLIP Tokenizier??\n",
    "- Step 1: Define the vocabulary of symbols that the BPE tokenizer will use to represent the text\n",
    "- Step 2: Takes a string of text as input and splits it into a list of symbols\n",
    "- Step 3: Calculates the frequency of each symbol pair in the text\n",
    "- Step 4: Sort the symbol pairs by frequency, with the most frequent pairs appearing first\n",
    "- Step 5: Implement a loop that repeatedly merges the most frequent symbol pair until a stopping condition is reached. This could be a fixed number of merges, or it could be based on the frequency of the symbol pairs (e.g., stop when the frequency of the most frequent pair drops below a certain threshold)\n",
    "- Step 6: As each symbol pair is merged, update the list of symbols and the symbol pair frequencies to reflect the changes.\n",
    "- Step 7: When the loop is finished, the resulting list of symbols will be the BPE vocabulary.\n",
    "- Step 8: Use the BPE vocabulary to encode text by replacing each symbol pair in the text with a single symbol from the vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_text(text):\n",
    "  # Compile a regular expression pattern to match any sequence of non-whitespace characters\n",
    "  pattern = re.compile(r'\\S+')\n",
    "  \n",
    "  # Use the pattern to split the text into a list of symbols\n",
    "  symbols = pattern.findall(text)\n",
    "  \n",
    "  return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "symbols = split_text(text)\n",
    "symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(symbols, ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_pair_frequencies():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pair_frequencies(symbols):\n",
    "  # Create an empty dictionary to store the symbol pair frequencies\n",
    "  pair_frequencies = {}\n",
    "  \n",
    "  # Iterate over the symbols and count the number of times each symbol pair appears in the text\n",
    "  for i in range(len(symbols) - 1):\n",
    "    pair = (symbols[i], symbols[i+1])\n",
    "    if pair in pair_frequencies:\n",
    "      pair_frequencies[pair] += 1\n",
    "    else:\n",
    "      pair_frequencies[pair] = 1\n",
    "  \n",
    "  return pair_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('The', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', 'jumps'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog.'): 1}\n"
     ]
    }
   ],
   "source": [
    "symbols = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n",
    "pair_frequencies = calculate_pair_frequencies(symbols)\n",
    "print(pair_frequencies)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "TOKEN_LENGTH = 77"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporary, i will steal the clip tokenizier from tinygrad. Will implement it from scratch later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@lru_cache()\n",
    "def default_bpe():\n",
    "  return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"../weights/bpe_simple_vocab_16e6.txt.gz\")\n",
    "  # return os.path.join(os.path.dirname(os.path.abspath(\".\")), \"../weights/bpe_simple_vocab_16e6.txt.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_pairs(word):\n",
    "  \"\"\"Return set of symbol pairs in a word.\n",
    "  Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "  \"\"\"\n",
    "  pairs = set()\n",
    "  prev_char = word[0]\n",
    "  for char in word[1:]:\n",
    "    pairs.add((prev_char, char))\n",
    "    prev_char = char\n",
    "  return pairs\n",
    "\n",
    "def whitespace_clean(text):\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  text = text.strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def bytes_to_unicode():\n",
    "  \"\"\"\n",
    "  Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "  The reversible bpe codes work on unicode strings.\n",
    "  This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "  When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "  This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "  To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "  And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "  \"\"\"\n",
    "  bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "  cs = bs[:]\n",
    "  n = 0\n",
    "  for b in range(2**8):\n",
    "    if b not in bs:\n",
    "      bs.append(b)\n",
    "      cs.append(2**8+n)\n",
    "      n += 1\n",
    "  cs = [chr(n) for n in cs]\n",
    "  return dict(zip(bs, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class ClipTokenizer:\n",
    "#   def __init__(self, bpe_path: str = default_bpe()):\n",
    "#     self.byte_encoder = bytes_to_unicode()\n",
    "#     merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
    "#     merges = merges[1:49152-256-2+1]\n",
    "#     merges = [tuple(merge.split()) for merge in merges]\n",
    "#     vocab = list(bytes_to_unicode().values())\n",
    "#     vocab = vocab + [v+'</w>' for v in vocab]\n",
    "#     for merge in merges:\n",
    "#       vocab.append(''.join(merge))\n",
    "#     vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
    "#     self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "#     self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "#     self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
    "#     self.pat = self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[^\\s]+\"\"\", re.IGNORECASE)\n",
    "\n",
    "#   def bpe(self, token):\n",
    "#     if token in self.cache:\n",
    "#       return self.cache[token]\n",
    "#     word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "#     pairs = get_pairs(word)\n",
    "\n",
    "#     if not pairs:\n",
    "#       return token+'</w>'\n",
    "\n",
    "#     while True:\n",
    "#       bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "#       if bigram not in self.bpe_ranks:\n",
    "#         break\n",
    "#       first, second = bigram\n",
    "#       new_word = []\n",
    "#       i = 0\n",
    "#       while i < len(word):\n",
    "#         try:\n",
    "#           j = word.index(first, i)\n",
    "#           new_word.extend(word[i:j])\n",
    "#           i = j\n",
    "#         except Exception:\n",
    "#           new_word.extend(word[i:])\n",
    "#           break\n",
    "\n",
    "#         if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "#           new_word.append(first+second)\n",
    "#           i += 2\n",
    "#         else:\n",
    "#           new_word.append(word[i])\n",
    "#           i += 1\n",
    "#       new_word = tuple(new_word)\n",
    "#       word = new_word\n",
    "#       if len(word) == 1:\n",
    "#         break\n",
    "#       else:\n",
    "#         pairs = get_pairs(word)\n",
    "#     word = ' '.join(word)\n",
    "#     self.cache[token] = word\n",
    "#     return word\n",
    "\n",
    "#   def encode(self, text):\n",
    "#     bpe_tokens = []\n",
    "#     text = whitespace_clean(text.strip()).lower()\n",
    "#     for token in re.findall(self.pat, text):\n",
    "#       token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "#       bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "#     # Truncation, keeping two slots for start and end tokens.\n",
    "#     if len(bpe_tokens) > 75:\n",
    "#       bpe_tokens = bpe_tokens[:75]\n",
    "#     return [49406] + bpe_tokens + [49407] * (77 - len(bpe_tokens) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizier = ClipTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"persistence is all you need\"\n",
    "# tokenizier.encode(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CLIPTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = transformers.CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    \n",
    "    @property\n",
    "    def model_max_length(self):\n",
    "        return self.tokenizer.model_max_length\n",
    "    \n",
    "    def encode(self, prompt: str):\n",
    "        # return self.tokenizer(prompt)\n",
    "        return self.tokenizer(prompt, padding=\"max_length\", max_length=self.tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.tokenizer(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizier = CLIPTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizier.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizier.encode(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
