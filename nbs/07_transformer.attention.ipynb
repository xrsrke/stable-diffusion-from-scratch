{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ae36ae-341f-4e62-b457-d9fd2c8e68c6",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea5a3d9-6da9-42e1-95ee-8999643cb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformer.attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7abfcf-90a7-4fa6-8770-f3637a1a4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667511a4-5760-412a-9385-1a231ab185cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c611dd-590e-4888-97a4-ff73528ed89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from fastcore.foundation import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b46e2-cb23-4ae2-895a-38530afd5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition(a,b):\n",
    "    \"Adds two numbers together\"\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc570b-00b3-41a8-9c9d-4e5c8b6c6d0b",
   "metadata": {},
   "source": [
    "#| explain a+b\n",
    "\n",
    "We take the sum of a and b, which is written in python with the \"+\" symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c750f6f-bb15-43d9-9e6c-4e2156033638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@docs\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    _docs = dict(cls_doc=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a9dcd-89d5-4aa8-a092-8732d872624a",
   "metadata": {},
   "source": [
    "dasdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0446962-a9f6-4af9-81e5-216cc59fbe28",
   "metadata": {},
   "source": [
    "#| explain \"pass\"\n",
    "\n",
    "We take the sum of a and b, which is written in python with the \"+\" symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd18c1-9dbd-498a-9d50-8ed0290d12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@docs\n",
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_k, bias):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_shape = x.shape[:-1]\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    _docs = dict(cls_doc=\"\",\n",
    "                 forward=\"yyy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c83c6-3680-4a41-87be-8be71e0e7956",
   "metadata": {},
   "source": [
    "#| explain \"self.heads = heads\"\n",
    "\n",
    "dasdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc790d77-c8a3-4823-a686-197abc0ff799",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761088e7-7a83-4ff7-b818-95c416532654",
   "metadata": {},
   "source": [
    "$\\operatorname{Attention}(q, k, v)=\\operatorname{softmax}\\left(\\frac{q k^T}{\\sqrt{d_k}}\\right) v$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dab4bb-cdbb-4bd5-8f71-6d90c10a4881",
   "metadata": {},
   "source": [
    "Given\n",
    "\n",
    "- $q$: the query vector\n",
    "- $k$: the key vector\n",
    "- $v$: the value vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e00df0-22c3-4b2c-a931-929c99a3ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _calculate_attention(q, k, v, mask=None):\n",
    "    d_k = k.shape[-1]\n",
    "\n",
    "    score = (q @ k.T) / math.sqrt(d_k)\n",
    "        \n",
    "    score = F.softmax(score, dim=-1)\n",
    "    attention_score = score @ v\n",
    "    \n",
    "    return attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29016b3-484c-4be3-88e5-fd22117c685e",
   "metadata": {},
   "source": [
    "#| explain \"score = (q @ k.T) / math.sqrt(d_k)\"\n",
    "\n",
    "Do dot product between vectors $q k^T$ and then divide by the dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef15ecb-1511-4dbb-b018-e44398a46103",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf0f32-260d-4399-abe3-52d17af20ec8",
   "metadata": {},
   "source": [
    "Suppose there're three words, each word has it owns query, key and value vectors. All vector have the same dimension - 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a11ab66-63b9-47ec-84ab-4897f4183f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, k1, v1 = torch.randn(5), torch.randn(5), torch.randn(5)\n",
    "q2, k2, v2 = torch.randn(5), torch.randn(5), torch.randn(5)\n",
    "q3, k3, v3 = torch.randn(5), torch.randn(5), torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1b323-83cc-4407-8fd1-bbd025747c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.stack([q1, q2, q3], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1957fa4-fb24-48b8-a1d8-5e725bcf5873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0566,  0.6788,  0.4330,  0.0026, -1.6254],\n",
       "        [-0.7342,  0.2160,  0.0464, -1.0750, -1.7149],\n",
       "        [ 2.2512, -0.8535,  0.1335, -0.0290, -0.8533]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64456457-9995-41f2-b8b4-0fef9c993602",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.stack([k1, k2, k3], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d37320-bcd0-4670-b719-e3a3cd85f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.stack([v1, v2, v3], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5bcca5-9257-4597-827d-cdeaa55a113c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0230, -1.6170, -1.1616,  0.6197,  0.1060],\n",
       "        [ 0.0221, -1.5535, -1.1491,  0.5655,  0.1212],\n",
       "        [ 0.1577, -0.6645, -1.3145,  0.2053,  0.0691]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_calculate_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96223bb0-ddfb-42e4-a64d-989c8c9c68e4",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a4b59-2578-4555-9fbe-3450326d06de",
   "metadata": {},
   "source": [
    "In practice, we don't compute each attention score at once, but we concentrate all the `key` to one matrix, same for `value` and `query`. That's why it called Multi-head attention. Just stack multiple attention layers and calcualte at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058750dd-6d3f-4500-951d-44419b84a412",
   "metadata": {},
   "source": [
    "$$\\operatorname{Attention}(Q, K, V)=\\underset{\\text { seq }}{\\operatorname{softmax}}\\left(\\frac{Q K^{\\top}}{\\sqrt{d_k}}\\right) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1e143-e538-40f6-9c4f-4ef98e8da8f4",
   "metadata": {},
   "source": [
    "- `d_model`: the number of features in `query`, `key`, and `value` vectors.\n",
    "- `n_head`: the number of attention layers.\n",
    "- `d_k`: the number of dimension of each vector in each head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11d37c-87b1-49d5-ab7f-866280d2a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| exports\n",
    "# @docs\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         heads: int,\n",
    "#         d_model: int,\n",
    "#         dropout_prop: float=0.1,\n",
    "#         bias: bool = True\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.d_k = d_model // heads\n",
    "        \n",
    "#         self.heads = heads\n",
    "        \n",
    "#         self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias)\n",
    "#         self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias)\n",
    "#         self.value = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias)\n",
    "        \n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "#         self.output = nn.Linear(d_model, d_model)\n",
    "#         self.dropout = nn.Dropout(dropout_prop)\n",
    "#         self.scale = 1 / math.sqrt(self.d_k)\n",
    "        \n",
    "#         self.attention = None\n",
    "    \n",
    "#     def get_scores(self, query, key):\n",
    "#         return self.query @ self.key.T\n",
    "        \n",
    "#     _docs = dict(cls_doc=\"Calculate the multi-head attention\",\n",
    "#                  get_scores=\"Calculate the score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a152d0-0eae-47ad-b84d-12f955629426",
   "metadata": {},
   "source": [
    "#### Okay. Then where do we get those query, key... vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a9cf6-b403-44e1-987a-391832c02223",
   "metadata": {},
   "source": [
    "So the key, value and query vector determines the attention score. We need someway to optimize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c82ca-883f-483e-b9d9-8d7956dadc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialize_weight(d_model):\n",
    "    return nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16126f64-4b8f-4043-aec9-e42a2b509e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| exports\n",
    "# def _split_by_heads(tensor, n_head):\n",
    "#     batch_size, length, d_model = tensor.size()\n",
    "#     d_tensor = d_model // n_head\n",
    "    \n",
    "#     return tensor.view(batch_size, length, n_head, d_tensor).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebab66-4b54-46d6-bd24-a9f69db917c0",
   "metadata": {},
   "source": [
    "#| explain \"d_tensor = d_model // n_head\"\n",
    "\n",
    "so here's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711d103-00ec-418a-a934-e3df7e67fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| exports\n",
    "# @docs\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_model, n_head):\n",
    "#         super().__init__()\n",
    "#         self.n_head = n_head\n",
    "    \n",
    "#         self.w_q = nn.Linear(d_model, d_model)\n",
    "#         self.w_k = nn.Linear(d_model, d_model)\n",
    "#         self.w_v = nn.Linear(d_model, d_model)\n",
    "            \n",
    "#     def split(self, tensor):\n",
    "#         batch_size, length, d_model = tensor.size()\n",
    "#         d_tensor = d_model // self.n_head\n",
    "        \n",
    "#         tensor = tensor.view(batch_size, length, self.n_head, d_tensor)\n",
    "#         tensor = tensor.transpose(1, 2)\n",
    "        \n",
    "#         return tensor\n",
    "\n",
    "#     def forward(self, q, k, v, mask = None):\n",
    "#         p, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        \n",
    "#         # 2. split tensor by number of heads\n",
    "#         q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "        \n",
    "#         # 3. do scale dot product to compute similarity\n",
    "#         out, attention = self.attention(q, k, v, mask=mask)\n",
    "        \n",
    "#         # 4. concat and pass to linear\n",
    "#         out = self.concat(out)\n",
    "#         out = self.w_concat(out)\n",
    "        \n",
    "#         return out\n",
    "    \n",
    "#     _docs = dict(cls_doc=\"\", split=\"\", forward=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e2988-c791-4646-b9e9-819985f04762",
   "metadata": {},
   "source": [
    "https://github.com/hyunwoongko/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13473fd1-7b86-4452-93ff-53f5c2312476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadAttention(d_model=5, n_head=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60558bf-992d-4999-8ee7-3dfed67687e7",
   "metadata": {},
   "source": [
    "### How multi-head attention works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cf1b0-215f-40ca-9047-5ca8eae915dd",
   "metadata": {},
   "source": [
    "Suppose the sentence: \"Persistent is all you need\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f82449-4880-4b92-b336-26f4f773e731",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Persistence is all you need\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b58f6-8f65-486e-96bd-6e835a3a98ff",
   "metadata": {},
   "source": [
    "There're five words in this sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf40f3d-5f13-40a5-875c-844f3e197f39",
   "metadata": {},
   "source": [
    "Each word represented by an vector has length `5` numbers in it (aka: dimension 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02759255-0ff3-4e53-891b-ae9e7a954c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = torch.randn(5), torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21dcb1-073b-4dc3-aae6-752ae079e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w3, w4, w5 = torch.randn(5), torch.randn(5), torch.randn(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1559b13-4fef-4fb0-a436-744214cdd20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2619,  0.4211,  0.6655,  1.5499,  0.9246])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952864c6-75ee-4685-81d7-5c5ec819d094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d9f76c8-3548-408d-8dc8-39922216da96",
   "metadata": {},
   "source": [
    "##### First we create a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed2c70-b8f7-4feb-9d9b-b5b2a25139f1",
   "metadata": {},
   "source": [
    "What i don't understand:\n",
    "- why each forward take `q`, `k`, `v`, not word?\n",
    "- what is `d_model`\n",
    "- why do split\n",
    "- why do concat\n",
    "- why dot w_concat\n",
    "- why attention return key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c1016-bcdd-4766-a1c9-e7a39db1a926",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6cca04-e784-4d31-9877-b8a686b1094d",
   "metadata": {},
   "source": [
    "- `num_heads`: this is the number of heads used in the multi-head attention operation. Each head performs attention on a different subset of the keys, values, and queries. \n",
    "- `d_model`:  the dimensionality of the input and output tensors in the multi-head attention operation\n",
    "- `d_k`: this is the dimensionality of the keys and values used in the multi-head attention operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8b6dc-43a1-4627-b657-803e251924eb",
   "metadata": {},
   "source": [
    "##### `d_k`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117bdd59-89c8-47db-8548-1d090da7b3bb",
   "metadata": {},
   "source": [
    "`d_k` is the dimensionality of the keys and values in the multi-head attention operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb72372-1515-40aa-9528-0fe8af5c9272",
   "metadata": {},
   "source": [
    "For example, if `d_model` is 256 and `num_heads` is 4, `d_k` would be 64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafba40-f1d0-4cfe-8aeb-fa09b8a49e2e",
   "metadata": {},
   "source": [
    "The reason for calculating `d_k` in this way is to ensure that the keys and values are split evenly among the different heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a148d-5d8e-4736-9108-baad92fbd030",
   "metadata": {},
   "source": [
    "Keys and values need to be split evenly among the different heads in a multi-head attention operation because each head will use its own set of keys and values to calculate the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ae27a-91b4-4687-a336-7204fd0c2fbc",
   "metadata": {},
   "source": [
    "##### `key` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d01b5a-c334-4d11-a374-4579e5d1a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c02c312-6dfe-4038-bd4f-6a23c57f8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_model, num_heads):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Save the number of heads and the dimensionality of the model.\n",
    "#         self.num_heads = num_heads\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#         # Calculate the dimensionality of the keys and values.\n",
    "#         self.d_k = d_model // num_heads\n",
    "\n",
    "#         # Create the linear layers for the keys, values, and queries.\n",
    "#         self.key_layer = nn.Linear(d_model, num_heads * self.d_k)\n",
    "#         self.value_layer = nn.Linear(d_model, num_heads * self.d_k)\n",
    "#         self.query_layer = nn.Linear(d_model, num_heads * self.d_k)\n",
    "\n",
    "#     def linear_layer(self, input_tensor):\n",
    "#         # Apply the linear layer to the input tensor to get the output.\n",
    "#         return self.linear_layer(input_tensor)\n",
    "\n",
    "#     def split_heads(self, input_tensor):\n",
    "#         # Split the input tensor into multiple heads along the last dimension.\n",
    "#         return input_tensor.reshape(input_tensor.shape[0], -1, self.num_heads, self.d_k)\n",
    "\n",
    "#     def dot_products(self, query, key):\n",
    "#         # Calculate the dot product of the query with the key for each head.\n",
    "#         return torch.einsum('bjhd,bkhd->bhjk', query, key)\n",
    "\n",
    "#     def scale_dot_products(self, dot_products):\n",
    "#         # Scale the dot products by the dimensionality of the keys.\n",
    "#         return dot_products / self.d_k**0.5\n",
    "\n",
    "#     def apply_weights(self, dot_products):\n",
    "#         # Apply the attention mask and softmax to the dot products to get the weights.\n",
    "#         return dot_products.softmax(dim=-1)\n",
    "\n",
    "#     def weighted_sum(self, weights, value):\n",
    "#         # Calculate the weighted sum of the values for each head.\n",
    "#         return torch.einsum('bhjk,bkhd->bjhd', weights, value)\n",
    "\n",
    "#     def concatenate_heads(self, output):\n",
    "#         # Concatenate the outputs from each head along the last dimension.\n",
    "#         return output.reshape(output.shape[0], -1, self.num_heads * self.d_k)\n",
    "\n",
    "#     def forward(self, query, key, value):\n",
    "#         # Transform the keys, values, and queries using the linear layers.\n",
    "#         key = self.key_layer(key)\n",
    "#         ipdb.set_trace()\n",
    "\n",
    "#         value = self.value_layer(value)\n",
    "#         query = self.query_layer(query)\n",
    "        \n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "#         # Split the keys, values, and queries into multiple heads.\n",
    "#         key = self.split_heads(key)\n",
    "#         value = self.split_heads(value)\n",
    "#         query = self.split_heads(query)\n",
    "        \n",
    "#         ipdb.set_trace()\n",
    "\n",
    "#         # Calculate the dot product of the query with the key for each head\n",
    "#         dot_products = self.dot_products(query, key)\n",
    "        \n",
    "#         ipdb.set_trace()\n",
    "\n",
    "#         # Scale the dot products by the dimensionality of the keys.\n",
    "#         dot_products = self.scale_dot_products(dot_products)\n",
    "        \n",
    "#         ipdb.set_trace()\n",
    "\n",
    "#         # Apply the attention mask and softmax to the dot products to get the weights.\n",
    "#         weights = self.apply_weights(dot_products)\n",
    "        \n",
    "#         ipdb.set_trace()\n",
    "\n",
    "#         # Calculate the weighted sum of the values for each head.\n",
    "#         output = self.weighted_sum(weights, value)\n",
    "\n",
    "#         # Concatenate the outputs from each head along the last dimension.\n",
    "#         output = self.concatenate_heads(output)\n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbcecef-ff69-4302-a46f-1d006c9f62ad",
   "metadata": {},
   "source": [
    "Suppose we have a four different sentence (aka: batch size), each "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67c7fe-a5cb-4144-b184-3d9766886b9a",
   "metadata": {},
   "source": [
    "For example, if the book has 1000 words and the word you are looking for has 5 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55fba9-43cd-4816-af30-7f25ba8eabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create some random tensors for the query, key, and value.\n",
    "# query_tensor = torch.randn(4, 5, 256)\n",
    "# key_tensor = torch.randn(4, 7, 256)\n",
    "# value_tensor = torch.randn(4, 7, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f6f18-d254-476b-949b-485208210a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a multi-head attention module with 4 heads and a dimensionality of 256.\n",
    "# attention = MultiHeadAttention(256, 4)\n",
    "\n",
    "# # Perform multi-head attention on some input tensors.\n",
    "# output = attention(query_tensor, key_tensor, value_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db926d-b2fa-4ed1-bc10-2d1991f79f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b6bca-372a-4b90-83e1-3460f6ec80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn(4, 5, 256) * torch.randn(256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0777ee-65a0-45b7-9df5-cbb484af29f6",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d930ae-4b43-421f-82d1-ad407e5c9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int = 4, num_heads: int = 2, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # d_q, d_k, d_v\n",
    "        self.d_h: int = d_model // num_heads\n",
    "\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ##create a list of layers for K, and a list of layers for V\n",
    "        self.linear_Qs = nn.ModuleList([nn.Linear(d_model, self.d_h)\n",
    "                                        for _ in range(num_heads)])\n",
    "        self.linear_Ks = nn.ModuleList([nn.Linear(d_model, self.d_h)\n",
    "                                        for _ in range(num_heads)])\n",
    "        self.linear_Vs = nn.ModuleList([nn.Linear(d_model, self.d_h)\n",
    "                                        for _ in range(num_heads)])\n",
    "\n",
    "        self.mha_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None):\n",
    "        # shape(Q) = [B x seq_len x D/num_heads]\n",
    "        # shape(K, V) = [B x seq_len x D/num_heads]\n",
    "\n",
    "        Q_K_matmul = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "        scores = Q_K_matmul/math.sqrt(self.d_h)\n",
    "        # shape(scores) = [B x seq_len x seq_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        # shape(attention_weights) = [B x seq_len x seq_len]\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        # shape(output) = [B x seq_len x D/num_heads]\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, pre_q, pre_k, pre_v, mask=None):\n",
    "        # shape(x) = [B x seq_len x D]\n",
    "\n",
    "        Q = [linear_Q(pre_q) for linear_Q in self.linear_Qs]\n",
    "        K = [linear_K(pre_k) for linear_K in self.linear_Ks]\n",
    "        V = [linear_V(pre_v) for linear_V in self.linear_Vs]\n",
    "        # shape(Q, K, V) = [B x seq_len x D/num_heads] * num_heads\n",
    "\n",
    "        output_per_head = []\n",
    "        attn_weights_per_head = []\n",
    "        # shape(output_per_head) = [B x seq_len x D/num_heads] * num_heads\n",
    "        # shape(attn_weights_per_head) = [B x seq_len x seq_len] * num_heads\n",
    "        \n",
    "        for Q_, K_, V_ in zip(Q, K, V):\n",
    "            \n",
    "            ##run scaled_dot_product_attention\n",
    "            output, attn_weight = self.scaled_dot_product_attention(Q_, K_, V_, mask)\n",
    "            # shape(output) = [B x seq_len x D/num_heads]\n",
    "            # shape(attn_weights_per_head) = [B x seq_len x seq_len]\n",
    "            output_per_head.append(output)\n",
    "            attn_weights_per_head.append(attn_weight)\n",
    "\n",
    "        output = torch.cat(output_per_head, -1)\n",
    "        attn_weights = torch.stack(attn_weights_per_head).permute(1, 0, 2, 3)\n",
    "        # shape(output) = [B x seq_len x D]\n",
    "        # shape(attn_weights) = [B x num_heads x seq_len x seq_len]\n",
    "        \n",
    "        projection = self.dropout(self.mha_linear(output))\n",
    "\n",
    "        return projection, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        batch_size, head, n_words, d_head = k.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967bcf4-4262-48b5-b469-a22b7853b748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:48:25) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d964ee95f874ee5744c8d970377d82f548475fce4b579cb2112551caf7b0ce8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
