{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ae36ae-341f-4e62-b457-d9fd2c8e68c6",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5a3d9-6da9-42e1-95ee-8999643cb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp transformer.attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7abfcf-90a7-4fa6-8770-f3637a1a4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667511a4-5760-412a-9385-1a231ab185cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c611dd-590e-4888-97a4-ff73528ed89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b46e2-cb23-4ae2-895a-38530afd5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition(a,b):\n",
    "    \"Adds two numbers together\"\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc570b-00b3-41a8-9c9d-4e5c8b6c6d0b",
   "metadata": {},
   "source": [
    "#| tip a+b\n",
    "\n",
    "We take the sum of a and b, which is written in python with the \"+\" symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c750f6f-bb15-43d9-9e6c-4e2156033638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class A:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4eba08-c476-488b-a74e-e95deef1c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.foundation import docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd18c1-9dbd-498a-9d50-8ed0290d12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@docs\n",
    "class PrepareForMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_k, bias):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, heads * d_k, bias=bias)\n",
    "        self.heads = heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_shape = x.shape[:-1]\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = x.view(*head_shape, self.heads, self.d_k)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    _docs = dict(cls_doc=\"xxx\",\n",
    "                 forward=\"yyy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c83c6-3680-4a41-87be-8be71e0e7956",
   "metadata": {},
   "source": [
    "#| explain \"self.heads = heads\"\n",
    "\n",
    "dasdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96223bb0-ddfb-42e4-a64d-989c8c9c68e4",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a4b59-2578-4555-9fbe-3450326d06de",
   "metadata": {},
   "source": [
    "In practice, we don't compute each attention score at once, but we concentrate all the `key` to one matrix, same for `value` and `query`. Then calculate all the attention score of these at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058750dd-6d3f-4500-951d-44419b84a412",
   "metadata": {},
   "source": [
    "$$\\operatorname{Attention}(Q, K, V)=\\underset{\\text { seq }}{\\operatorname{softmax}}\\left(\\frac{Q K^{\\top}}{\\sqrt{d_k}}\\right) V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1e143-e538-40f6-9c4f-4ef98e8da8f4",
   "metadata": {},
   "source": [
    "- `d_model`: the number of features in `query`, `key`, and `value` vectors.\n",
    "- `heads`: the number of attention layers.\n",
    "- `d_k`: the number of dimension of each vector in each head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11d37c-87b1-49d5-ab7f-866280d2a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        heads: int,\n",
    "        d_model: int,\n",
    "        dropout_prop: float=0.1,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711d103-00ec-418a-a934-e3df7e67fb5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14d928-6a36-448f-ab77-4d4ed2baae48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
