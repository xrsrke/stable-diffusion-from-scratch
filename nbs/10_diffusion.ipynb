{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18ed250-1687-47dd-9475-c617c5624314",
   "metadata": {},
   "source": [
    "# diffusion\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453fb90-81e2-427d-9ab7-d9661344165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa87288-6645-4a55-a74f-4878a9ba390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3b7f4-2178-4c4d-9406-79921c0f10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d87ba9-d40b-46ec-b211-f8f2ec2b913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/education/miniforge3/envs/sb-from-scratch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from matplotlib import pyplot as plt\n",
    "from fastcore.foundation import L\n",
    "\n",
    "from foundation.utils import show_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a526d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist_dataset = torchvision.datasets.MNIST(root=\"mnist/\", train=True, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9486c-85d2-491f-9fb6-4e858a68e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(mnist_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ac36a-6e11-47b4-bb1d-49e7eb866588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter(train_dataloader))\n",
    "# print('Input shape:', x.shape)\n",
    "# print('Labels:', y)\n",
    "# plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c5848-d664-45e9-9da5-7ca9a8b57a5b",
   "metadata": {},
   "source": [
    "### Why Diffusion Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82144baf-e80f-424a-8be2-fdb28a9e850f",
   "metadata": {},
   "source": [
    "The idea of diffusion is we sequentially add more noise to an image and then lets the model to predict what's noise it need to remove to get back the original image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f33938-ed83-479d-843e-9062612cb9ff",
   "metadata": {},
   "source": [
    "### Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24340995-b4b3-4c91-bc6f-ba652a29a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = next(iter(train_dataloader))[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7b685-9bfc-492d-9cb4-77c06fe0fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_img(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c464df-23cc-49c7-8371-4fca2bf4a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = torch.randn_like(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a03bb-2634-45f1-9ea6-be3a68f0335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_img(img + noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a2a73b-1915-44fa-b06b-88cf19883578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# def corrupt(img, amount):\n",
    "#     noise = torch.randn_like(img)\n",
    "#     return (1-amount)*img + noise * amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc27b6-50f3-4da2-8cf9-353e6111bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amounts = torch.linspace(0, 1, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28540722-28b3-4fee-96f6-3336bca22e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c045a47-43f2-4727-8742-99f55108801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupt_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34dd2a8-59bd-40d2-97a5-32b3c43b728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for amount in amounts:\n",
    "#     # ipdb.set_trace()\n",
    "#     corrupt_images.append(corrupt(img, amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e941cf-27bf-4135-a609-21aaa4aa1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for corrupt_imgage in corrupt_images:\n",
    "#     show_img(corrupt_imgage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77a1fe3-97e4-44d7-94c5-92c9b78e096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.utils.make_grid(corrupt_images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad8186-5bac-48f5-8b5d-72811c5f12cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "# axs.set_title('Input data')\n",
    "# axs.imshow(torchvision.utils.make_grid(corrupt_images)[0], cmap='Greys')\n",
    "# axs.imshow(torchvision.utils.make_grid(corrupt_images)[1], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f998a80-b1c4-4140-805c-c7b3f00b0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axs.imshow(torchvision.utils.make_grid(corrupt_images)[5], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19386316-7252-475c-973a-2f79d40d3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# class BasicUNet(nn.Module):\n",
    "#     \"\"\"A minimal UNet implementation.\"\"\"\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.down_layers = torch.nn.ModuleList([ \n",
    "#             nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\n",
    "#             nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "#             nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "#         ])\n",
    "#         self.up_layers = torch.nn.ModuleList([\n",
    "#             nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "#             nn.Conv2d(64, 32, kernel_size=5, padding=2),\n",
    "#             nn.Conv2d(32, out_channels, kernel_size=5, padding=2), \n",
    "#         ])\n",
    "#         self.act = nn.SiLU()\n",
    "#         self.downscale = nn.MaxPool2d(2)\n",
    "#         self.upscale = nn.Upsample(scale_factor=2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = []\n",
    "#         for i, l in enumerate(self.down_layers):\n",
    "#             x = self.act(l(x))\n",
    "#             h.append(x)\n",
    "#             if i < 2: x = self.downscale(x)\n",
    "#         for i, l in enumerate(self.up_layers):\n",
    "#             if i > 0: x = self.upscale(x)\n",
    "#             x += h.pop()\n",
    "#             x = self.act(l(x))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet = BasicUNet(1, 2)\n",
    "# # unet(torch.rand(8, 1, 28, 28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = BasicUNet(1, 1)\n",
    "\n",
    "# # Feed some data through:\n",
    "# x.shape, net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# # The optimizer - explore different learning rates or try\n",
    "# # a different optimizer instead\n",
    "# opt = torch.optim.Adam(net.parameters(), lr=3e-4) \n",
    "\n",
    "# # Keeping a record of the losses for later viewing\n",
    "# losses = []\n",
    "\n",
    "# # And a record of smoothed loss values after each epoch\n",
    "# smoothed_losses_basic = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e945b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb-from-scratch",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
