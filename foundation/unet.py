# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/09_unet.ipynb.

# %% auto 0
__all__ = ['BasicUNet']

# %% ../nbs/09_unet.ipynb 4
import torch
from torch import nn

# %% ../nbs/09_unet.ipynb 5
class BasicUNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1):
        super().__init__()
        self.down_layers = nn.ModuleList([
            nn.Conv2d(
                in_channels=in_channels, out_channels=32,
                kernel_size=5, padding=2
            ),
            nn.Conv2d(
                in_channels=32, out_channels=64,
                kernel_size=5, padding=2
            ),
            nn.Conv2d(
                in_channels=64, out_channels=64,
                kernel_size=5, padding=2
            ),
            
        ])
        
        self.up_layers = nn.ModuleList([
            nn.Conv2d(
                in_channels=64, out_channels=64,
                kernel_size=5, padding=2
            ),
            nn.Conv2d(
                in_channels=64, out_channels=32,
                kernel_size=5, padding=2
            ),
            nn.Conv2d(
                in_channels=32, out_channels=out_channels,
                kernel_size=5, padding=2
            ),
        ])
        self.act = nn.SiLU()
        self.downscale = nn.MaxPool2d(kernel_size=2)
        self.upscale = nn.Upsample(scale_factor=2)
    
    def forward(self, x):
        h = []
        for i, layer in enumerate(self.down_layers):
            x = self.act(layer(x))
            if i < 2:
                h.append(x)
                x = self.downscale(x)
        
        for i, layer in enumerate(self.up_layers):
            if i > 0:
                x = self.upscale(x)
                x += h.pop()
            x = self.act(layer(x))
        
        return x
