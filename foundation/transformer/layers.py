# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/07_transformer.encoder.ipynb.

# %% auto 0
__all__ = ['ResidualLayer', 'EncoderLayer']

# %% ../../nbs/07_transformer.encoder.ipynb 4
import torch
from torch import nn

# %% ../../nbs/07_transformer.encoder.ipynb 5
class ResidualLayer(nn.Module):
    def __init__(self, d_model, dropout=0.3):
        super().__init__()
        self.layers = nn.Sequential(
            nn.LayerNorm(normalized_shape=d_model),
            nn.Dropout(p=dropout)
        )
    
    def forward(self, x, residual):
        return self.layers(x, residual)

# %% ../../nbs/07_transformer.encoder.ipynb 6
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.3):
        super().__init__()
        self.norm_1 = ResidualLayer(d_model, dropout)
        self.mha = None
        self.norm_2 = ResidualLayer(d_model, dropout)
        self.positional_wise = None
    
    def forward(self, x):
        
        # shape(mha) = [batch_size x seq_len x d_model]
        # shape(encoder_attention_weights) = [batch_size x num_heads x seq_len x seq_len]
        mha, encoder_attention_weights = self.mha(x, x, x, mask=mask)
        
        # shape(norm1) = [batch_size x seq_len x d_model]
        norm_1 = self.norm_1(mha, x)
        
        # shape(positional_wise) = [batch_size x seq_len x d_model]
        positional_wise = self.positional_wise(norm_1)
        
        # shape(norm_2) = [batch_size x seq_len x d_model]
        norm_2 = self.norm_2(positional_wise, norm_1)
        
        return norm_2, encoder_attention_weights
