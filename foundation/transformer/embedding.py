# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/transformer/07e_transformer.embedding.ipynb.

# %% auto 0
__all__ = ['TextEmbedding']

# %% ../../nbs/transformer/07e_transformer.embedding.ipynb 4
import math
from typing import Union

import torch
from torch import nn

# %% ../../nbs/transformer/07e_transformer.embedding.ipynb 7
class TextEmbedding(nn.Module):
    def __init__(self, vocab_size: int, d_model: int, padding_idx: Union[int, float]):
        super().__init__()
        self.d_model = d_model
        self.embeding = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)
    
    def forward(self, tokens: torch.Tensor):
        # shape(tokens) = [batch_size x seq_len]
        
        # shape(embedding) = [batch_size x seq_len x d_model]
        embeddings = self.embeding(tokens)
        return embeddings * math.sqrt(self.d_model)
