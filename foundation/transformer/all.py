# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/transformer/07_transformer.ipynb.

# %% auto 0
__all__ = ['Transformer']

# %% ../../nbs/transformer/07_transformer.ipynb 4
import torch
from torch import nn

from .encoder import Encoder
from .decoder import Decoder
from .embedding import TextEmbedding
from .positional_encoding import PositionalEncoding

# %% ../../nbs/transformer/07_transformer.ipynb 11
class Transformer(nn.Module):
    def __init__(
        self,
        src_vocab_len: int, trg_vocab_len: int,
        d_model: int, d_ff: int,
        n_layers: int, n_heads: int,
        src_pad_idx: int, trg_pad_idx: int,
        dropout: float = 0.3,
    ):
        super().__init__()
        
        self.n_heads = n_heads
        
        encoder_embedding = TextEmbedding(
            vocab_size=src_vocab_len,
            padding_idx=src_pad_idx,
            d_model=d_model
        )
        decoder_embedding = TextEmbedding(
            vocab_size=trg_vocab_len,
            padding_idx=trg_pad_idx,
            d_model=d_model
        )
        
        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        
        self.encoder = Encoder(d_model, n_heads, n_layers, d_ff, dropout)
        self.decoder = Decoder(d_model, n_heads, n_layers, d_ff, dropout)
        
        self.linear_layer = nn.Linear(d_model, trg_vocab_len)
    
    def create_src_mask(self, src: torch.Tensor) -> torch.Tensor:
        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        return src_mask
    
    def create_trg_mask(self, trg: torch.Tensor) -> torch.Tensor:
        seq_len = trg.shape[1]
        
        trg_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)
        mask = torch.ones((1, self.n_heads, seq_len, seq_len)).triu(1)
        
        mask = mask == 0
        trg_mask = trg_mask & mask
        
        return trg_mask
    
    def forward(self, src: torch.Tensor, trg: torch.Tensor) -> torch.Tensor:
        
        # shape(src) = [batch_size x srg_seq_len]
        # shape(trg) = [batch_size x trg_seq_len]
        
        # shape(src_mask) = [batch_size x 1 x src_seq_len]
        src_mask = self.create_src_mask(src)
        
        # shape(trg_mask) = [batch_size x 1 x trg_seq_len]
        trg_mask = self.create_trg_mask(trg)
        
        # shape(encoder_output) = [batch_size x src_seq_len x d_model]
        # shape(encoder_mha_attention_weights) = [batch_size x n_heads x src_seq_len x src_seq_len]
        encoder_output, encoder_mha_attention_weights = self.encoder(src, src_mask)
        
        # shape(decoder_output) = [batch_size x trg_seq_len x d_model]
        # shape(decoder_mha_attention_weights) = [batch_size x n_heads x trg_seq_len x src_seq_len]
        decoder_output, _, decoder_mha_attention_weights = self.decoder(trg, encoder_output, trg_mask, src_mask)
        
        # shape(logits) = [batch_size x targ_seq_len x targ_vocab_size]
        logits = self.linear_layer(decoder_output)
        return logits
