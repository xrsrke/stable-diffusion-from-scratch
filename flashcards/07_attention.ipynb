{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1992e5a-94d5-400a-993b-b25ab38b0a83",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0c06a-b0b9-4b66-a401-893c309a4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387514c2-a8cd-4f97-aae6-7c6eb031a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b20a2f1-6f92-4a89-99a9-f2bc869b1346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/nbdev/export.py:54: UserWarning: Notebook '/Users/study/DATA/projects/ai/stable-diffusion-from-scratch/nbs/07b_attention.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe09020f-17fd-41a9-8370-22aa890f1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7082d9-56ba-4e54-9499-670c9f749c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baad771-6414-4df1-972d-89928dec89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d91880-5bc6-4fb3-834a-16bed4adae41",
   "metadata": {},
   "source": [
    "#### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe4c61-8983-4ac0-ac4d-460b9c45395d",
   "metadata": {},
   "source": [
    "the sentence would need to be converted into numerical representation, such as a vector or a tensor, that can be processed by the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65765f73-e747-4f97-889a-ae0e5317bf95",
   "metadata": {},
   "source": [
    "The sentence \"Persistence is all you need\" could be represented as a tensor with dimensions `(5, d)`, where `d` is the dimensionality of the word vectors. The tensor would contain the word vectors for each of the words in the sentence, in the same order as they appear in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c9b4a-c119-4cb2-8eaf-652e5a64e3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentence to be converted\n",
    "sentence = \"persistence is all you need\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b7b57-bf73-4632-833e-49a9e8335a0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [61], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m word_vectors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m----> 8\u001b[0m     word_vector \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39membed(token)\n\u001b[1;32m      9\u001b[0m     word_vectors\u001b[38;5;241m.\u001b[39mappend(word_vector)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Convert the list of word vectors into a tensor\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# with dimensions (sequence_length, d)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentence into individual words\n",
    "tokens = sentence.split()\n",
    "\n",
    "# Embed each token into a word vector\n",
    "# using a pre-trained word embedding model\n",
    "word_vectors = []\n",
    "for token in tokens:\n",
    "    word_vector = model.embed(token)\n",
    "    word_vectors.append(word_vector)\n",
    "\n",
    "# Convert the list of word vectors into a tensor\n",
    "# with dimensions (sequence_length, d)\n",
    "sequence_length = len(tokens)\n",
    "d = word_vectors[0].size(0)\n",
    "tensor = torch.stack(word_vectors, dim=0)\n",
    "tensor = tensor.view(sequence_length, d)\n",
    "\n",
    "# The resulting tensor can now be used as input\n",
    "# to a machine learning model, such as the multi_head_attention function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739d5b9-fc46-4632-8761-8ebd8cc0a98e",
   "metadata": {},
   "source": [
    "#### Split heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab329e88-aa49-451f-8354-5dca975e4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads, d_k):\n",
    "    # Split the tensor along the last dimension\n",
    "    # into num_heads tensors of shape (batch_size, sequence_length, d_k)\n",
    "    x = x.view(x.size(0), x.size(1), num_heads, d_k)\n",
    "    return x.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e89963b-4908-4681-9e75-4a2bd1edb057",
   "metadata": {},
   "source": [
    "#| explain \"x = x.view(x.size(0), x.size(1), num_heads, d_k)\"\n",
    "\n",
    "`x.size(0), x.size(1)`: keep the batch_size and sequence length of a tensor and `num_heads`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcacbfe-c5ef-49ff-96b4-d4b650a7d0f7",
   "metadata": {},
   "source": [
    "#| explain \"return x.permute(0, 2, 1, 3)\"\n",
    "\n",
    "Change the shape of tensor to use (`batch_size`, `num_heads`, `sequence_length`, `d_k`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10a324-0697-41eb-9d69-81c48de4e832",
   "metadata": {},
   "source": [
    "#### Combine heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e096ab-5f01-41cf-b315-328576f01c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_heads(x, num_heads, d_k):\n",
    "    # Combine the tensors along the last two dimensions\n",
    "    # into a tensor of shape (batch_size, sequence_length, d_model)\n",
    "    x = x.permute(0, 2, 1, 3).contiguous()\n",
    "    return x.view(x.size(0), x.size(1), num_heads * d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3092e-ab45-469a-bd72-083c96e2de19",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89efecb8-cab0-4c6f-bf87-97aeccbdce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, d_k):\n",
    "    # Calculate the dot product attention\n",
    "    scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef0d73-56f8-4594-ae85-ff670774c385",
   "metadata": {},
   "source": [
    "#| explain \"torch.matmul(query, key.transpose(-1, -2))\"\n",
    "\n",
    "In order to compute the dot product between the `query` and `key` tensors, the `key` tensor needs to be transposed so that its dimensions match the dimensions of the `query` tensor. Specifically, the `key` tensor needs to be transposed along the last two dimensions, so that its dimensions become `(batch_size, d_k, sequence_length)`. Producing an output tensor of dimensions `(batch_size, sequence_length, sequence_length)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a67cb5-58b6-4083-9a2b-94bbf6772840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(d_model, num_heads, query_tensor, key_tensor, value_tensor):\n",
    "    d_k = d_model // num_heads\n",
    "\n",
    "    key_layer = nn.Linear(d_model, num_heads * d_k)\n",
    "    value_layer = nn.Linear(d_model, num_heads * d_k)\n",
    "    query_layer = nn.Linear(d_model, num_heads * d_k)\n",
    "\n",
    "    # Apply the linear layers to the input tensors\n",
    "    key_tensor = key_layer(key_tensor)\n",
    "    value_tensor = value_layer(value_tensor)\n",
    "    query_tensor = query_layer(query_tensor)\n",
    "\n",
    "    # Split the tensors into multiple heads\n",
    "    key_tensor = split_heads(key_tensor, num_heads, d_k)\n",
    "    value_tensor = split_heads(value_tensor, num_heads, d_k)\n",
    "    query_tensor = split_heads(query_tensor, num_heads, d_k)\n",
    "\n",
    "    # Apply attention to each head\n",
    "    attention_output = attention(query_tensor, key_tensor, value_tensor, d_k)\n",
    "\n",
    "    # Combine the attention output from each head\n",
    "    attention_output = combine_heads(attention_output, num_heads, d_k)\n",
    "\n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe31095-eb45-4cf1-ad68-b1819624c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "d_model = 256\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93956f-102e-4033-92cb-49d2a1904101",
   "metadata": {},
   "source": [
    "- `query_tensor`: (batch_size=4, sequence_length=5, d_model=256)\n",
    "- `key_tensor`: (batch_size=4, sequence_length=7, d_model=256)\n",
    "- `value_tensor`: (batch_size=4, sequence_length=7, d_model=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a46851-7674-473b-b49d-af9212bb4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some random input tensors\n",
    "query_tensor = torch.randn(4, 5, d_model)\n",
    "key_tensor = torch.randn(4, 7, d_model)\n",
    "value_tensor = torch.randn(4, 7, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c244c8-7ce2-4e34-adce-cf368d372529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# Apply multi-head attention\n",
    "attention_output = multi_head_attention(d_model, num_heads, query_tensor, key_tensor, value_tensor)\n",
    "\n",
    "# Print the output shape\n",
    "print(attention_output.shape)  # should be (4, 5, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd15ca-b974-407a-a568-5eadc0819e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c9a9876-8f5b-4812-9b8c-5e81e8d878c6",
   "metadata": {},
   "source": [
    "### Multi-Head Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454d1cb-de7f-4e2f-bef4-e9af5167237d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d964ee95f874ee5744c8d970377d82f548475fce4b579cb2112551caf7b0ce8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
