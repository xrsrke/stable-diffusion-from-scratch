{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815d064f-09be-4373-b7a8-27df030f45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2183f1e-1ca4-440e-b775-61090274c536",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c38274-8e89-4fe4-9240-9487e507db51",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a2c6c3-d9f9-420b-8d61-bff212d8b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from foundation.transformer.encoder import ResidualLayerNorm, PostionWiseFeedForward\n",
    "from foundation.transformer.efficient_attention import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08fd419a-64ee-4890-ba32-26b72963d0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DecoderLayer(nn.ModuleList):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.norm_1 = ResidualLayerNorm(d_model)\n",
    "        self.norm_2 = ResidualLayerNorm(d_model)\n",
    "        self.norm_3 = ResidualLayerNorm(d_model)\n",
    "        \n",
    "        self.masked_mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.encoder_decoder_mha = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = PostionWiseFeedForward(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x, encoder_outputs, trg_mask, src_mask):\n",
    "        # shape(x) = [batch_size x trg_seq_len x d_model]\n",
    "        # shape(encoder_output) = [batch_size x src_seq_len x d_model]\n",
    "        \n",
    "        \n",
    "        # shape(masked_mha) = [batch_size x trg_se_len x d_model]\n",
    "        # shape(masked_mha_attn_weights) \n",
    "        # = [batch_size x n_heads x trg_seq_len x trg_seq_len]\n",
    "        masked_mha, masked_mha_attn_weights = self.masked_mha(x, x, x, mask=trg_mask)\n",
    "        \n",
    "        norm_1 = self.norm_1(masked_mha, x)\n",
    "        \n",
    "        # shape(encoder_decoder_mha) = [batch_size x trg_seq_len x d_model]\n",
    "        # shape(encoder_decoder_mha_attn_weights) = [batch_size x n_heads x trg_seq_len x trg_seq_len]\n",
    "        encoder_decoder_mha, encoder_decoder_mha_attn_weights = self.encoder_decoder_mha(\n",
    "            pre_q=norm_1, pre_k=encoder_outputs, pre_v=encoder_outputs,\n",
    "            mask=src_mask\n",
    "        )\n",
    "        \n",
    "        norm_2 = self.norm_2(encoder_decoder_mha, norm_1)\n",
    "        feed_forward = self.feed_forward(norm_2)\n",
    "        norm_3 = self.norm_3(feed_forward, norm_2)\n",
    "        return norm_3, masked_mha_attn_weights, encoder_decoder_mha_attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87bd86f-1f38-49b2-a92e-6cedcd7af797",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = DecoderLayer(d_model=10, n_heads=2, d_ff=16, dropout=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb5786-4fc2-4c9c-a0c2-5ed2487a14d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
