{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6253cb-11de-4d4c-8564-e915b9cc7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ad099-05ed-45ad-9bc8-bf30e95f3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, d_k = 4):\n",
    "    QK = torch.matmul(Q, K.T)\n",
    "\n",
    "    matmul_scaled = QK / math.sqrt(d_k)\n",
    "\n",
    "    attention_weights = F.softmax(matmul_scaled, dim=-1)\n",
    "    \n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a9ea6-6061-40ea-82bf-f118fbca2730",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_k = torch.Tensor([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]])  # (4, 3)\n",
    "\n",
    "temp_v = torch.Tensor([[   1,0, 1],\n",
    "                      [  10,0, 2],\n",
    "                      [ 100,5, 0],\n",
    "                      [1000,6, 0]])  # (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e7357-dcb6-475b-b29f-8bd5af73c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_q = torch.Tensor([[0, 10, 0]])  # (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1c278-c4b8-47dd-85cd-51e6e7b704a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000e+01, 2.1216e-21, 2.0000e+00]]),\n",
       " tensor([[1.9287e-22, 1.0000e+00, 1.9287e-22, 1.9287e-22]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b725996-563f-4971-8839-7f1a1f0e25f8",
   "metadata": {},
   "source": [
    "##### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a006d2-62d5-44bf-9786-df674439d961",
   "metadata": {},
   "source": [
    "`scaled_dot_product_attention` is a function calculate the self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d7918-0123-480b-a86b-e7f9aa6abd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_q = torch.Tensor([[0, 10, 0], [0, 0, 10], [10, 10, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73d192-6e25-4a06-8f6d-b41c58fa2156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.,  0.,  0.],\n",
       "         [ 0., 10.,  0.],\n",
       "         [ 0.,  0., 10.],\n",
       "         [ 0.,  0., 10.]]),\n",
       " tensor([[   1.,    0.,    1.],\n",
       "         [  10.,    0.,    2.],\n",
       "         [ 100.,    5.,    0.],\n",
       "         [1000.,    6.,    0.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_k, temp_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c1642-7a07-4eae-9e44-58986f97ed8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 10.,  0.],\n",
       "        [ 0.,  0., 10.],\n",
       "        [10., 10.,  0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb48c7-f84f-44df-b2bc-0faab3195efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, attention_weights = scaled_dot_product_attention(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97cac72-6c5c-4fb5-8c0c-56c2b207ea7b",
   "metadata": {},
   "source": [
    "Iterept the output of `attention_weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab3fb2e-a16b-46dd-a06a-85a0aec811c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(attention_weights, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056bcdf4-d65a-453b-921d-a32e7660a1b2",
   "metadata": {},
   "source": [
    "**Explain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae28ab3-e945-44e8-a415-9114f41bede2",
   "metadata": {},
   "source": [
    "Each row in `attention_weights` corresponds to how similar each vector in `temp_q` similar to each vector in `temp_k`\n",
    "- The first row: `1.000` at index 1st means => `temp_q[0]` similars to `temp_k[1]`\n",
    "- The second row: `0.5000` and `0.5000` at index 2nd and 3th and  means => `temp_q[1]` similars to `temp_k[2]` and `temp_k[3]`\n",
    "- The third row: `0.5000` and `0.5000` at index 0th and 1st and  means => `temp_q[2]` similars to `temp_k[0]` and `temp_k[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6535bb-5662-4170-9599-8c54ed83ad83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(attention_weights, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a402ac-ddde-4b6c-a7fd-f564db561f14",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576614ac-ab88-49c9-b14b-41b27dd2bda3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=4, num_heads=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # calculate the dimensionality per head\n",
    "        self.d_h = d_model // num_heads\n",
    "        \n",
    "        assert self.d_h * num_heads == d_model\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # go from d_model to d per head\n",
    "        self.linear_qs = nn.ModuleList([\n",
    "            nn.Linear(d_model, self.d_h) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear_ks = nn.ModuleList([\n",
    "            nn.Linear(d_model, self.d_h) for _ in range(num_heads)\n",
    "        ])        \n",
    "        self.linear_vs = nn.ModuleList([\n",
    "            nn.Linear(d_model, self.d_h) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        # shape(Q, K, V) = [batch_size x seq_len x d_h] * num_heads\n",
    "        # shape(Q) = [batch_size x seq_len x d_h]\n",
    "        # shape(K) = [batch_size x seq_len x d_h] => [batch_size x d_h x seq_len]\n",
    "        \n",
    "        # shape(Q_K_matmul) = [batch_size x seq_len x seq_len]\n",
    "        Q_K_matmul = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "        \n",
    "        # shape(scores) = [batch_size x seq_len x seq_len]\n",
    "        scores = Q_K_matmul / math.sqrt(self.d_h)\n",
    "        \n",
    "        # shape(attn_weights) = [batch_size x seq_len x seq_len]\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # shape(output) = [batch_size x seq_len x d_h]\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        return output, attn_weights\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # shape(x) = [batch_size x seq_len x d_model]\n",
    "        \n",
    "        # shape(Q, K, V) = [batch_size x seq_len x d_h] * num_heads\n",
    "        Q = [linear_q(x) for linear_q in self.linear_qs]\n",
    "        K = [linear_k(x) for linear_k in self.linear_ks]\n",
    "        V = [linear_v(x) for linear_v in self.linear_vs]\n",
    "        \n",
    "        # shape(output_per_head) = [batch_size x seq_len x d_h] * num_heads\n",
    "        output_per_head = []\n",
    "        \n",
    "        # shape(attn_weight_per_head) = [batch_size x seq_len x seq_len] * num_heads\n",
    "        attn_weight_per_head = []\n",
    "        \n",
    "        for Q_, K_, V_ in zip(Q, K, V):\n",
    "            output, attn_weight = self.scaled_dot_product_attention(Q_, K_, V_)\n",
    "            output_per_head.append(output)\n",
    "            attn_weight_per_head.append(attn_weight)\n",
    "        \n",
    "        # shape(output) = [batch_size x seq_len x d_model]\n",
    "        output = torch.cat(output_per_head, dim=-1)\n",
    "        \n",
    "        # shape(attn_weights) = [num_heads x batch_size x seq_len x seq_len]\n",
    "        attn_weights = torch.stack(attn_weight_per_head)\n",
    "        \n",
    "        # shape(attn_weights) = [batch_size x num_heads x seq_len x seq_len]\n",
    "        attn_weights = attn_weights.permute(1, 0, 2, 3)\n",
    "        \n",
    "        projection = self.dropout(self.linear(output))\n",
    "        \n",
    "        return projection, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab31ed1-a46b-40c2-9c5a-d66e6afc68f9",
   "metadata": {},
   "source": [
    "- **Step 1**: `x` is the input text has shape `[batch_size x seq_len x D]`\n",
    "\n",
    "- **Step 2**: After `x` go through `Q` => shape `[batch_size x seq_len x d_per_head] * num_heads`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239d8b7-1499-45ee-a564-856e01f6c6e9",
   "metadata": {},
   "source": [
    "Notations\n",
    "- `d_model`: the model dimensionality\n",
    "- `d_h`: the head dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddb89b-b3c0-494d-9cd9-86cbdbddcb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_encodings = torch.Tensor([[\n",
    "    [0.0, 0.1, 0.2, 0.3],\n",
    "    [1.0, 1.1, 1.2, 1.3],\n",
    "    [2.0, 2.1, 2.2, 2.3]\n",
    "]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d06a7e-359e-4006-9788-ad000a2c365a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2196f803-ee45-4f8b-b779-f84cb58db857",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(d_model=4, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd403cfd-46fd-4ccd-ae73-4c13bbea52d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output, attn_weights\u001b[38;5;241m=\u001b[39m \u001b[43mmha\u001b[49m(toy_encodings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mha' is not defined"
     ]
    }
   ],
   "source": [
    "output, attn_weights= mha(toy_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98a764-a2b7-4a9f-8234-5c77220a092b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f3e2a-0882-4605-9f0d-8628a357d3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284945e1-2a9c-4f24-ab6f-24461ffbda45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
